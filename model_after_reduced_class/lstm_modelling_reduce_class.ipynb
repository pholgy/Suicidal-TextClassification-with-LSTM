{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import keras\n",
    "from keras import *\n",
    "from keras.layers import *\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.word_vector import *\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from pythainlp import word_vector\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "    patience=8, \n",
    "    min_delta=0.001, \n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('main_suicidal_data.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wModel = word_vector.WordVector(model_name=\"thai2fit_wv\").get_model()\n",
    "thai2dict = {}\n",
    "for word in wModel.index_to_key:\n",
    "    thai2dict[word] = wModel[word]\n",
    "thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')\n",
    "thVocab = thai2vec.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36100"
     ]
    }
   ],
   "source": [
    "ll = len(thai2vec)\n",
    "for vidx in range(ll):\n",
    "    if vidx % 100 == 0:\n",
    "        print('\\r' + str(vidx),end='')\n",
    "    aa = thai2vec.iloc[[vidx]]\n",
    "    ab = aa.values.tolist()\n",
    "    if vidx == 0:\n",
    "        vect = ab\n",
    "    else:\n",
    "        vect = np.vstack((vect,ab))\n",
    "\n",
    "print(\"\\n\", vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenWord(wordTarget):\n",
    "    wordToken = word_tokenize(wordTarget, engine='attacut')\n",
    "    return wordToken\n",
    "\n",
    "def convWord(cw):\n",
    "    cWord = cw\n",
    "    for ti in range(len(cWord)):\n",
    "        if cWord[ti] == ' ':\n",
    "            cWord[ti] = ''\n",
    "        elif cWord[ti] not in thVocab:\n",
    "            cWord[ti] = ''\n",
    "    return cWord\n",
    "\n",
    "def token2index(t2idx):\n",
    "    w2index = []\n",
    "    for wi in range(len(t2idx)):\n",
    "        if t2idx[wi] in thVocab:\n",
    "            w2index.append(thVocab.index(t2idx[wi]))\n",
    "    return np.array(w2index)\n",
    "\n",
    "def findMaxArray(fma):\n",
    "    maxlen = 0\n",
    "    for mi in range(len(fma)):\n",
    "        nA = len(fma[mi])\n",
    "        if nA > maxlen:\n",
    "            maxlen = nA\n",
    "    return maxlen\n",
    "\n",
    "def fill0in(f0i):\n",
    "    fMax = findMaxArray(f0i)\n",
    "    for fi, ax in enumerate(f0i):\n",
    "        if len(ax) < fMax:\n",
    "            f0i[fi] = np.hstack((ax , np.zeros(fMax-len(ax))))\n",
    "        f0i1 = np.array(f0i)\n",
    "    return f0i1\n",
    "\n",
    "def prepare2train(ipt):\n",
    "    pre2t = []\n",
    "    for pidx in range(len(ipt)):\n",
    "        wp1 = ipt[pidx]\n",
    "        pre2t.append(token2index(convWord(tokenWord(wp1))))\n",
    "    return pre2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label_Enc'] = df['Label (Specialist)'].str.replace('Level 1','Low Level Depress')\n",
    "df['Label_Enc'] = df['Label_Enc'].str.replace('Level 2','Mid Level Depress')\n",
    "df['Label_Enc'] = df['Label_Enc'].str.replace('Level 3','Mid Level Depress')\n",
    "df['Label_Enc'] = df['Label_Enc'].str.replace('Level 4','High Level Depress')\n",
    "df['Label_Enc'] = df['Label_Enc'].str.replace('Level 5','High Level Depress')\n",
    "\n",
    "conditions = [\n",
    "    (df['Label_Enc'] == 'Other'),\n",
    "    (df['Label_Enc'] == 'Low Level Depress'),\n",
    "    (df['Label_Enc'] == 'Mid Level Depress'),\n",
    "    (df['Label_Enc'] == 'High Level Depress'),\n",
    "    ]\n",
    "\n",
    "values = ['0', '1', '2', '3']\n",
    "\n",
    "df['Label_Enc'] = np.select(conditions, values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Tweet']\n",
    "X_arr = X.to_list()\n",
    "y = df['Label_Enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prepare2train(X_arr)\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X = fill0in(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(input_dim=51358,output_dim=300,name='embed'))\n",
    "\n",
    "#Layer 2: LSTM layer\n",
    "model.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "model.add(Dropout(rate=0.6))\n",
    "#Layer 3: LSTM layer\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(rate=0.6))\n",
    "\n",
    "#Layer 4: Dense layer (Hidden layer)\n",
    "model.add(Dense(24, activation = 'relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "\n",
    "#Layer 5: Output layer\n",
    "model.add(Dense(6, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.001,momentum=0.9,nesterov=True)\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer=adam_opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('embed').set_weights([vect])\n",
    "model.get_layer('embed').trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,epochs=20,batch_size=32,callbacks=[early_stopping],validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "def plot_metric(history, metric):\n",
    "    train_metrics = history.history[metric]\n",
    "    val_metrics = history.history['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics)\n",
    "    plt.plot(epochs, val_metrics)\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(X_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:25:34) \n[Clang 11.1.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "312e11345a26bb658cc76d44d3920b107a1290315a78b9f69927adb21ad3c257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
