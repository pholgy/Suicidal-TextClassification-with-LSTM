{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import *\n",
    "from keras.layers import *\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.word_vector import *\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Suicidal_K5_Train.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wModel = get_model()\n",
    "thai2dict = {}\n",
    "for word in wModel.index_to_key:\n",
    "    thai2dict[word] = wModel[word]\n",
    "thai2vec = pd.DataFrame.from_dict(thai2dict,orient='index')\n",
    "thVocab = thai2vec.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai2vec.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = len(thai2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vidx in range(ll):\n",
    "    if vidx % 100 == 0:\n",
    "        print('\\r' + str(vidx),end='')\n",
    "    aa = thai2vec.iloc[[vidx]]\n",
    "    ab = aa.values.tolist()\n",
    "    if vidx == 0:\n",
    "        vect = ab\n",
    "    else:\n",
    "        vect = np.vstack((vect,ab))\n",
    "\n",
    "print(\"\\n\", vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2num(convData):\n",
    "    for ci in range(len(convData)):\n",
    "        if convData[ci] == \"Level 1\":\n",
    "            convData[ci] = 0\n",
    "        elif convData[ci] == \"Level 2\":\n",
    "            convData[ci] = 1\n",
    "        elif convData[ci] == \"Level 3\":\n",
    "            convData[ci] = 2\n",
    "        elif convData[ci] == \"Level 4\":\n",
    "            convData[ci] = 3\n",
    "        elif convData[ci] == \"Level 5\":\n",
    "            convData[ci] = 4\n",
    "        elif convData[ci] == \"Other\":\n",
    "            convData[ci] = 5\n",
    "    return convData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenWord(wordTarget):\n",
    "    wordToken = word_tokenize(wordTarget, engine='attacut')\n",
    "    return wordToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convWord(cw):\n",
    "    cWord = cw\n",
    "    for ti in range(len(cWord)):\n",
    "        if cWord[ti] == ' ':\n",
    "            cWord[ti] = ''\n",
    "        elif cWord[ti] not in thVocab:\n",
    "            cWord[ti] = ''\n",
    "    return cWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index(t2idx):\n",
    "    w2index = []\n",
    "    for wi in range(len(t2idx)):\n",
    "        if t2idx[wi] in thVocab:\n",
    "            w2index.append(thVocab.index(t2idx[wi]))\n",
    "    return np.array(w2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxArray(fma):\n",
    "    maxlen = 0\n",
    "    for mi in range(len(fma)):\n",
    "        nA = len(fma[mi])\n",
    "        if nA > maxlen:\n",
    "            maxlen = nA\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill0in(f0i):\n",
    "    fMax = findMaxArray(f0i)\n",
    "    for fi, ax in enumerate(f0i):\n",
    "        if len(ax) < fMax:\n",
    "            f0i[fi] = np.hstack((ax , np.zeros(fMax-len(ax))))\n",
    "        f0i1 = np.array(f0i)\n",
    "    return f0i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare2train(ipt):\n",
    "    pre2t = []\n",
    "    for pidx in range(len(ipt)):\n",
    "        wp1 = ipt[pidx]\n",
    "        pre2t.append(token2index(convWord(tokenWord(wp1))))\n",
    "    return pre2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = df['Tweet']\n",
    "tweetArray = tweet.to_list()\n",
    "label = df['Label (Specialist)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = prepare2train(tweetArray)\n",
    "Ytrain = convert2num(label.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTrainIdx = np.array(Ytrain)\n",
    "XTrainIdx = fill0in(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainIdx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vect.shape[0],vect.shape[1],name='embed'))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('embed').set_weights([vect])\n",
    "model.get_layer('embed').trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.95):\n",
    "      self.model.stop_training = True\n",
    "      print(\"\\nCallback Accuracy มากกว่า 95%!\")\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(XTrainIdx,YTrainIdx,epochs=50,batch_size=32,callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('Suicidal_K5_Test.csv').drop(columns='Unnamed: 0')\n",
    "test_label = test_df['Label (Specialist)']\n",
    "X_test = test_df['Tweet']\n",
    "\n",
    "test_array = X_test.to_list()\n",
    "\n",
    "X_test = prepare2train(test_array)\n",
    "y_test = convert2num(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTestIdx = np.array(y_test)\n",
    "XTestIdx = fill0in(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(XTestIdx),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df['Label (Specialist)']\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTestIdx = np.asarray(XTestIdx).astype(np.float32)\n",
    "YTestIdx = np.asarray(YTestIdx).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(XTestIdx,YTestIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.tolist()\n",
    "y_pred = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "thesis_nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "312e11345a26bb658cc76d44d3920b107a1290315a78b9f69927adb21ad3c257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
